<?xml version="1.0" encoding="UTF-8" ?>
<ChoregrapheProject xmlns="http://www.aldebaran-robotics.com/schema/choregraphe/project.xsd" xar_version="3">
    <Box name="root" id="-1" localization="8" tooltip="Root box of Choregraphe&apos;s behavior. Highest level possible." x="0" y="0">
        <bitmap>media/images/box/root.png</bitmap>
        <script language="4">
            <content>
                <![CDATA[]]>
</content>
        </script>
        <Input name="onLoad" type="1" type_size="1" nature="0" inner="1" tooltip="Signal sent when diagram is loaded." id="1" />
        <Input name="onStart" type="1" type_size="1" nature="2" inner="0" tooltip="Box behavior starts when a signal is received on this input." id="2" />
        <Input name="onStop" type="1" type_size="1" nature="3" inner="0" tooltip="Box behavior stops when a signal is received on this input." id="3" />
        <Output name="onStopped" type="1" type_size="1" nature="1" inner="0" tooltip="Signal sent when box behavior is finished." id="4" />
        <Timeline enable="0">
            <BehaviorLayer name="behavior_layer1">
                <BehaviorKeyframe name="keyframe1" index="1">
                    <Diagram>
                        <Box name="Watson Dialog" id="7" localization="8" tooltip="Enter description here" x="102" y="18">
                            <bitmap>media/images/box/watson-lib-icons/speech_icon.png</bitmap>
                            <script language="4">
                                <content>
                                    <![CDATA[]]>
</content>
                            </script>
                            <Input name="onLoad" type="1" type_size="1" nature="0" inner="1" tooltip="Signal sent when diagram is loaded." id="1" />
                            <Input name="Watson Dialog Init_onStart" type="1" type_size="1" nature="2" inner="0" tooltip="This input has been automatically generated&#x0A;by converting several boxes into a single box." id="2" />
                            <Parameter name="Instance ID" inherits_from_parent="0" content_type="3" value="48" default_value="Enter Chat ID" custom_choice="1" tooltip="This is the Cognea instance ID for a dialog." id="3">
                                <Choice value="Enter Chat ID" />
                            </Parameter>
                            <Timeline enable="0">
                                <BehaviorLayer name="behavior_layer1">
                                    <BehaviorKeyframe name="keyframe1" index="1">
                                        <Diagram scale="70.7107">
                                            <Box name="Watson Dialog Ask" id="2" localization="8" tooltip="This box uses Cognea, a service for aiding in conversation. It continues to send user input to Cognea, and it recieves the corresponding responses.&#x0A;&#x0A;Input: It takes the user&apos;s side of the conversation as a String input. &#x0A;&#x0A;Output: The output is the response to the user&apos;s response as a String." x="232" y="251">
                                                <bitmap>media/images/box/watson-lib-icons/watson_icon.png</bitmap>
                                                <script language="4">
                                                    <content>
                                                        <![CDATA[class MyClass(GeneratedClass):
    def __init__(self):
        GeneratedClass.__init__(self)
        self.watson = ALProxy('ALWatson')

    def onLoad(self):
        #put initialization code here
        pass

    def onUnload(self):
        #put clean-up code here
        pass

    def onInput_onStart(self, p):
        response = self.watson.ask(p)
        self.onStopped(response)]]>
</content>
                                                </script>
                                                <Input name="onLoad" type="1" type_size="1" nature="0" inner="1" tooltip="Signal sent when diagram is loaded." id="1" />
                                                <Input name="onStart" type="3" type_size="1" nature="2" inner="0" tooltip="The input is the user&apos;s side of the conversation. It is a response to the robot&apos;s side of the dialog." id="2" />
                                                <Output name="onStopped" type="3" type_size="1" nature="1" inner="0" tooltip="The output is the robot&apos;s side of the conversation. It is a response to the user&apos;s side of the dialog." id="3" />
                                                <Parameter name="Instance ID" inherits_from_parent="1" content_type="3" value="48" default_value="" custom_choice="1" tooltip="" id="4" />
                                            </Box>
                                            <Box name="Animated Say Text" id="4" localization="8" tooltip="Say the text received on its input and move during its speech." x="195" y="0">
                                                <bitmap>media/images/box/interaction/say.png</bitmap>
                                                <script language="4">
                                                    <content>
                                                        <![CDATA[import time

class MyClass(GeneratedClass):
    def __init__(self):
        GeneratedClass.__init__(self, False)
        self.animSpeech = ALProxy('ALAnimatedSpeech')

    def onLoad(self):
        self.bIsRunning = False
        self.ids = []

    def onUnload(self):
        for id in self.ids:
            try:
                self.animSpeech.stop(id)
            except:
                pass

    def onInput_onStart(self, p):
        self.bIsRunning = True
        try:
            sentence = "\RSPD="+ str( self.getParameter("Speed (%)") ) + "\ "
            sentence += "\VCT="+ str( self.getParameter("Voice shaping (%)") ) + "\ "
            sentence += str(p)
            sentence +=  "\RST\ "
            configuration =\
            {"bodyLanguageMode":self.getParameter("Body language mode")}
            id = self.animSpeech.post.say(str(sentence), configuration)
            self.ids.append(id)
            self.animSpeech.wait(id, 0)
        finally:
            try:
                self.ids.remove(id)
            except:
                pass
            if( self.ids == [] ):
                self.onStopped() # activate output of the box
                self.bIsRunning = False

    def onInput_onStop(self):
        self.onUnload()]]>
</content>
                                                </script>
                                                <Input name="onLoad" type="1" type_size="1" nature="0" inner="1" tooltip="Signal sent when Diagram is loaded." id="1" />
                                                <Input name="onStart" type="3" type_size="1" nature="2" inner="0" tooltip="Box behavior starts when a signal is received on this Input." id="2" />
                                                <Input name="onStop" type="1" type_size="1" nature="3" inner="0" tooltip="Box behavior stops when a signal is received on this Input." id="3" />
                                                <Output name="onStopped" type="1" type_size="1" nature="1" inner="0" tooltip="Signal sent when Box behavior is finished." id="4" />
                                                <Parameter name="Voice shaping (%)" inherits_from_parent="1" content_type="1" value="100" default_value="100" min="50" max="150" tooltip='Used to modify at runtime the voice feature (tone, speed). In a slighty&#x0A;different way than pitch and speed, it gives a kind of &quot;gender or age&#x0A;modification&quot; effect.&#x0A;&#x0A;For instance, a quite good male derivation of female voice can be&#x0A;obtained setting this parameter to 78%.&#x0A;&#x0A;Note: For a better effect, you can compensate this parameter with the&#x0A;speed parameter. For example, if you want to decrease by 20% the voice&#x0A;shaping, you will have to increase by 20% the speed to keep a constant&#x0A;average speed.' id="5" />
                                                <Parameter name="Speed (%)" inherits_from_parent="1" content_type="1" value="100" default_value="100" min="50" max="200" tooltip="Changes the speed of the voice.&#x0A;&#x0A;Note: For a better effect, you can compensate this parameter with the voice&#x0A;shaping parameter. For example, if you want to increase by 20% the speed, you&#x0A;will have to decrease by 20% the voice shaping to keep a constant average&#x0A;speed." id="6" />
                                                <Parameter name="Body language mode" inherits_from_parent="1" content_type="3" value="contextual" default_value="contextual" custom_choice="0" tooltip="Change the body language mode during the speech.&#x0A;disabled: The robot will only play the animations given by the user. ex: ^start(...) or ^tag(...)&#x0A;random: During time the robot has no animation to play, he will launch random neutral animations.&#x0A;contextual: During time the robot has no animation to play, he will try to launch a new one accordingly to the saying text. Every time the robot can&apos;t find a contextual animation he will launch a random neutral animation." id="7">
                                                    <Choice value="disabled" />
                                                    <Choice value="random" />
                                                    <Choice value="contextual" />
                                                </Parameter>
                                            </Box>
                                            <Box name="Watson Dialog Init" id="6" localization="8" tooltip="This box uses Cognea, a service for aiding in conversation. It connects to an instance on Cognea, and it will access that dialog file." x="53" y="0">
                                                <bitmap>media/images/box/watson-lib-icons/watson_icon.png</bitmap>
                                                <script language="4">
                                                    <content>
                                                        <![CDATA[class MyClass(GeneratedClass):
    def __init__(self):
        GeneratedClass.__init__(self)
        self.watson = ALProxy('ALWatson')

    def onLoad(self):
        #put initialization code here
        pass

    def onUnload(self):
        #put clean-up code here
        pass

    def onInput_onStart(self):
        speech = self.watson.initialize_chat(self.getParameter('Instance ID'))
        self.onStopped(speech)]]>
</content>
                                                </script>
                                                <Input name="onLoad" type="1" type_size="1" nature="0" inner="1" tooltip="Signal sent when diagram is loaded." id="1" />
                                                <Input name="onStart" type="1" type_size="1" nature="2" inner="0" tooltip="Box behavior starts when a signal is received on this input." id="2" />
                                                <Output name="onStopped" type="3" type_size="1" nature="1" inner="0" tooltip="This is the initial response returned by Cognea to let the user know when it is ready for conversation." id="3" />
                                                <Parameter name="Instance ID" inherits_from_parent="1" content_type="3" value="41" default_value="Enter Chat ID" custom_choice="1" tooltip="" id="4">
                                                    <Choice value="Enter Chat ID" />
                                                </Parameter>
                                            </Box>
                                            <Box name="Animated Say Text (1)" id="5" localization="8" tooltip="Say the text received on its input and move during its speech." x="376" y="245">
                                                <bitmap>media/images/box/interaction/say.png</bitmap>
                                                <script language="4">
                                                    <content>
                                                        <![CDATA[import time

class MyClass(GeneratedClass):
    def __init__(self):
        GeneratedClass.__init__(self, False)
        self.animSpeech = ALProxy('ALAnimatedSpeech')

    def onLoad(self):
        self.bIsRunning = False
        self.ids = []

    def onUnload(self):
        for id in self.ids:
            try:
                self.animSpeech.stop(id)
            except:
                pass

    def onInput_onStart(self, p):
        self.bIsRunning = True
        try:
            sentence = "\RSPD="+ str( self.getParameter("Speed (%)") ) + "\ "
            sentence += "\VCT="+ str( self.getParameter("Voice shaping (%)") ) + "\ "
            sentence += str(p)
            sentence +=  "\RST\ "
            configuration =\
            {"bodyLanguageMode":self.getParameter("Body language mode")}
            id = self.animSpeech.post.say(str(sentence), configuration)
            self.ids.append(id)
            self.animSpeech.wait(id, 0)
        finally:
            try:
                self.ids.remove(id)
            except:
                pass
            if( self.ids == [] ):
                self.onStopped() # activate output of the box
                self.bIsRunning = False

    def onInput_onStop(self):
        self.onUnload()]]>
</content>
                                                </script>
                                                <Input name="onLoad" type="1" type_size="1" nature="0" inner="1" tooltip="Signal sent when Diagram is loaded." id="1" />
                                                <Input name="onStart" type="3" type_size="1" nature="2" inner="0" tooltip="Box behavior starts when a signal is received on this Input." id="2" />
                                                <Input name="onStop" type="1" type_size="1" nature="3" inner="0" tooltip="Box behavior stops when a signal is received on this Input." id="3" />
                                                <Output name="onStopped" type="1" type_size="1" nature="1" inner="0" tooltip="Signal sent when Box behavior is finished." id="4" />
                                                <Parameter name="Voice shaping (%)" inherits_from_parent="1" content_type="1" value="100" default_value="100" min="50" max="150" tooltip='Used to modify at runtime the voice feature (tone, speed). In a slighty&#x0A;different way than pitch and speed, it gives a kind of &quot;gender or age&#x0A;modification&quot; effect.&#x0A;&#x0A;For instance, a quite good male derivation of female voice can be&#x0A;obtained setting this parameter to 78%.&#x0A;&#x0A;Note: For a better effect, you can compensate this parameter with the&#x0A;speed parameter. For example, if you want to decrease by 20% the voice&#x0A;shaping, you will have to increase by 20% the speed to keep a constant&#x0A;average speed.' id="5" />
                                                <Parameter name="Speed (%)" inherits_from_parent="1" content_type="1" value="100" default_value="100" min="50" max="200" tooltip="Changes the speed of the voice.&#x0A;&#x0A;Note: For a better effect, you can compensate this parameter with the voice&#x0A;shaping parameter. For example, if you want to increase by 20% the speed, you&#x0A;will have to decrease by 20% the voice shaping to keep a constant average&#x0A;speed." id="6" />
                                                <Parameter name="Body language mode" inherits_from_parent="1" content_type="3" value="contextual" default_value="contextual" custom_choice="0" tooltip="Change the body language mode during the speech.&#x0A;disabled: The robot will only play the animations given by the user. ex: ^start(...) or ^tag(...)&#x0A;random: During time the robot has no animation to play, he will launch random neutral animations.&#x0A;contextual: During time the robot has no animation to play, he will try to launch a new one accordingly to the saying text. Every time the robot can&apos;t find a contextual animation he will launch a random neutral animation." id="7">
                                                    <Choice value="disabled" />
                                                    <Choice value="random" />
                                                    <Choice value="contextual" />
                                                </Parameter>
                                            </Box>
                                            <Box name="Watson Listening" id="25" localization="8" tooltip="This box makes the robot listen for human speech in English, and transcribes it. This is done by using the Bluemix Speech to Text (STT) service.&#x0A;&#x0A;Input: A signal to tell the box when to start listening.&#x0A;&#x0A;Output: A string of text, which is a transcription of what the robot heard." x="324" y="49">
                                                <bitmap>media/images/box/watson-lib-icons/speech_icon.png</bitmap>
                                                <script language="4">
                                                    <content>
                                                        <![CDATA[]]>
</content>
                                                </script>
                                                <Input name="onLoad" type="1" type_size="1" nature="0" inner="1" tooltip="Signal sent when diagram is loaded." id="1" />
                                                <Input name="Record Sound_onStart" type="1" type_size="1" nature="2" inner="0" tooltip="This input has been automatically generated&#x0A;by converting several boxes into a single box." id="2" />
                                                <Output name="WatsonListening_onStopped" type="3" type_size="1" nature="1" inner="0" tooltip="This output has been automatically generated&#x0A;by converting several boxes into a single box." id="3" />
                                                <Timeline enable="0">
                                                    <BehaviorLayer name="behavior_layer1">
                                                        <BehaviorKeyframe name="keyframe1" index="1">
                                                            <Diagram>
                                                                <Box name="Watson Listening" id="5" localization="8" tooltip="This box makes the robot listen for human speech in English, and transcribes it. This is done by using the Bluemix Speech to Text (STT) service.&#x0A;&#x0A;Input: A signal to tell the box when to start listening.&#x0A;&#x0A;Output: A string of text, which is a transcription of what the robot heard." x="364" y="4">
                                                                    <bitmap>media/images/box/watson-lib-icons/speech_icon.png</bitmap>
                                                                    <script language="4">
                                                                        <content>
                                                                            <![CDATA[class MyClass(GeneratedClass):
    def __init__(self):
        GeneratedClass.__init__(self)
        self.watson = ALProxy('ALWatson')


    def onLoad(self):
        #put initialization code here
        pass

    def onUnload(self):
        #put clean-up code here
        pass

    def onInput_onStart(self):
        speech = self.watson.stt()
        self.onStopped(speech) #activate the output of the box]]>
</content>
                                                                    </script>
                                                                    <Input name="onLoad" type="1" type_size="1" nature="0" inner="1" tooltip="Signal sent when diagram is loaded." id="1" />
                                                                    <Input name="onStart" type="1" type_size="1" nature="2" inner="0" tooltip="A signal to notify the box when it needs to start listening." id="2" />
                                                                    <Output name="onStopped" type="3" type_size="1" nature="1" inner="0" tooltip="A string of text that is the transcription of what the robot heard." id="3" />
                                                                </Box>
                                                                <Box name="Record Sound" id="24" localization="8" tooltip='Record a sound on the robot. You can choose to record only with one&#x0A;microphone (the front head microphone) in ogg format, or with four microphones&#x0A;(front, sides and rear head microphones) in wav format.&#x0A;&#x0A;If &quot;Temporary storage&quot; isn&apos;t checked, the output sound file is located in &quot;~/recordings/microphones/&lt;File name&gt;&quot;.&#x0A;Else, it is located in a temporary directory&#x0A;&#x0A;The onStopped output is stimulated at the end of the recording and contains the absolute path to the output sound file&#x0A;&#x0A;V1.1.0&#x0A;' x="143" y="0">
                                                                    <bitmap>media/images/box/interaction/rec_sound.png</bitmap>
                                                                    <script language="4">
                                                                        <content>
                                                                            <![CDATA[]]>
</content>
                                                                    </script>
                                                                    <Input name="onLoad" type="1" type_size="1" nature="0" inner="1" tooltip="Signal sent when diagram is loaded." id="1" />
                                                                    <Input name="onStart" type="1" type_size="1" nature="2" inner="0" tooltip="Launches the recording of the sound." id="2" />
                                                                    <Input name="onStop" type="1" type_size="1" nature="3" inner="0" tooltip="Stops the recording of the sound." id="3" />
                                                                    <Output name="onStopped" type="3" type_size="1" nature="1" inner="0" tooltip="Returns the absolute path of the output sound file at the end of the recording" id="4" />
                                                                    <Parameter name="File name" inherits_from_parent="0" content_type="3" value="recording" default_value="recording" custom_choice="0" tooltip="Name of the destination file without its extension." id="5" />
                                                                    <Parameter name="Microphones used" inherits_from_parent="0" content_type="3" value="Front, sides and rear head microphones (.wav)" default_value="Front, sides and rear head microphones (.wav)" custom_choice="0" tooltip="Microphones used to record the sound.&#x0A;&#x0A;Note: If you use only the front head microphone the file will be saved in ogg format. If you use the&#x0A;front, sides and rear head microphones it will be saved in wav format." id="6">
                                                                        <Choice value="Front head microphone only (.ogg)" />
                                                                        <Choice value="Front, sides and rear head microphones (.wav)" />
                                                                    </Parameter>
                                                                    <Parameter name="Temporary storage" inherits_from_parent="0" content_type="0" value="0" default_value="1" tooltip="Choose if the file should be stored as a temporary file so it is removed as soon&#x0A;as the behavior is unloaded.&#x0A;&#x0A;When this option is disabled the file is stored in ~/recordings/microphones. You can&#x0A;then get it on your computer using the menu Connection/File transfer.&#x0A;&#x0A;When this option is enabled the file is stored in the temporary folder of the behavior." id="7" />
                                                                    <Parameter name="Timeout (s)" inherits_from_parent="0" content_type="2" value="3" default_value="5" min="0.1" max="60" tooltip="Duration of the recording in seconds." id="8" />
                                                                    <Timeline enable="0">
                                                                        <BehaviorLayer name="behavior_layer1">
                                                                            <BehaviorKeyframe name="keyframe1" index="1">
                                                                                <Diagram>
                                                                                    <Box name="Rec. Sound File" id="4" localization="8" tooltip="Record a sound on the robot. You can choose to record only with one&#x0A;microphone (the front head microphone) in ogg format, or with four microphones&#x0A;(front, sides and rear head&#x0A;microphones) in wav format." x="562" y="100">
                                                                                        <bitmap>media/images/box/interaction/rec_sound.png</bitmap>
                                                                                        <script language="4">
                                                                                            <content>
                                                                                                <![CDATA[class MyClass(GeneratedClass):
    def __init__(self):
        GeneratedClass.__init__(self, False)
        try:
            self.ad = ALProxy("ALAudioDevice")
        except Exception as e:
            self.ad = None
            self.logger.error(e)
        self.filepath = ""

    def onLoad(self):
        self.bIsRecording = False
        self.bIsRunning = False

    def onUnload(self):
        self.bIsRunning = False
        if( self.bIsRecording ):
            self.ad.stopMicrophonesRecording()
            self.bIsRecording = False

    def onInput_onStart(self, p):
        if(self.bIsRunning):
            return
        self.bIsRunning = True
        sExtension = self.toExtension( self.getParameter("Microphones used") )
        self.filepath = p + sExtension
        if self.ad:
            self.ad.startMicrophonesRecording( self.filepath )
            self.bIsRecording = True
        else:
            self.logger.warning("No sound recorded")

    def onInput_onStop(self):
        if( self.bIsRunning ):
            self.onUnload()
            self.onStopped(self.filepath)

    def toExtension(self, sMicrophones):
        if( sMicrophones == "Front head microphone only (.ogg)" ):
            return ".ogg"
        else:
            return ".wav"]]>
</content>
                                                                                        </script>
                                                                                        <Input name="onLoad" type="1" type_size="1" nature="0" inner="1" tooltip="Signal sent when diagram is loaded." id="1" />
                                                                                        <Input name="onStart" type="3" type_size="1" nature="2" inner="0" tooltip="Launches the recording of the sound. The data received on this input must be the&#x0A;filename of the sound." id="2" />
                                                                                        <Input name="onStop" type="1" type_size="1" nature="3" inner="0" tooltip="Stops the recording of the sound." id="3" />
                                                                                        <Output name="onStopped" type="3" type_size="1" nature="1" inner="0" tooltip="Returns the absolute path of the output sound file at the end of the recording" id="4" />
                                                                                        <Parameter name="Microphones used" inherits_from_parent="1" content_type="3" value="Front, sides and rear head microphones (.wav)" default_value="Front, sides and rear head microphones (.wav)" custom_choice="0" tooltip="Microphones used to record the sound.&#x0A;&#x0A;Note: If you use only the front head microphone the file will be saved in ogg format. If you use the&#x0A;front, sides and rear head microphones it will be saved in wav format." id="5">
                                                                                            <Choice value="Front head microphone only (.ogg)" />
                                                                                            <Choice value="Front, sides and rear head microphones (.wav)" />
                                                                                        </Parameter>
                                                                                    </Box>
                                                                                    <Box name="Wait" id="13" localization="8" tooltip="Wait a moment before sending a bang on the output. The wait can be stopped any&#x0A;time. You may restart it any time, and it will start over." x="420" y="161">
                                                                                        <bitmap>media/images/box/wait.png</bitmap>
                                                                                        <script language="4">
                                                                                            <content>
                                                                                                <![CDATA[import threading # opkg install python-threading
class MyClass(GeneratedClass):
    def __init__(self):
        GeneratedClass.__init__(self, False)

    def onLoad(self):
        self.tb = False

    def onUnload(self):
        if( self.tb ):
            self.tb.cancel()
            self.tb = False

    def triggerOutput( self, rPeriod = 1 ):
        self.onUnload() # to clean everything
        self.timerOutput()

    def startTimer( self, rPeriod = 1 ):
        if( self.tb ):
            self.tb.cancel()
        self.tb = threading.Timer( rPeriod, self.triggerOutput, [rPeriod] )
        self.tb.start()

    def onInput_onStart(self):
        self.startTimer( self.getParameter("Timeout (s)") )

    def onInput_onStop(self):
        bRunning = self.tb
        self.onUnload()
        if( bRunning ):
            self.timerOutput()]]>
</content>
                                                                                        </script>
                                                                                        <Input name="onLoad" type="1" type_size="1" nature="0" inner="1" tooltip="Signal sent when diagram is loaded." id="1" />
                                                                                        <Input name="onStart" type="1" type_size="1" nature="2" inner="0" tooltip="Start the Wait box with the configured timeout value." id="2" />
                                                                                        <Input name="onStop" type="1" type_size="1" nature="3" inner="0" tooltip="Stop the wait and stimulate the output." id="3" />
                                                                                        <Output name="timerOutput" type="1" type_size="1" nature="1" inner="0" tooltip="Send a bang once time set in parameters is ellapsed, or if the box is stopped." id="4" />
                                                                                        <Parameter name="Timeout (s)" inherits_from_parent="1" content_type="2" value="5" default_value="1" min="0" max="5000" tooltip="Duration the box waits before stimulating the output." id="5" />
                                                                                    </Box>
                                                                                    <Box name="Get File Name" id="10" localization="8" tooltip="Use this box to choose an attached file in its parameters. The filename will be sent on&#x0A;the output when the input is stimulated." x="234" y="95">
                                                                                        <bitmap>media/images/box/folder.png</bitmap>
                                                                                        <script language="4">
                                                                                            <content>
                                                                                                <![CDATA[import os
class MyClass(GeneratedClass):
    def __init__(self):
        GeneratedClass.__init__(self, False)

    def onLoad(self):
        self.framemanager = ALProxy("ALFrameManager")

    def onInput_onStart(self):
        if( self.getParameter("Temporary storage") ):
            import tempfile
            path = tempfile.mkdtemp() + "/"
        else:
            path = os.path.expanduser('~') + "/recordings/microphones/"
        self.onStopped( path + self.getParameter("File name") )]]>
</content>
                                                                                        </script>
                                                                                        <Input name="onLoad" type="1" type_size="1" nature="0" inner="1" tooltip="Signal sent when diagram is loaded." id="1" />
                                                                                        <Input name="onStart" type="1" type_size="1" nature="1" inner="0" tooltip="To send the filepath on the output." id="2" />
                                                                                        <Output name="onStopped" type="3" type_size="1" nature="2" inner="0" tooltip="The filepath of the selected resource file." id="3" />
                                                                                        <Parameter name="File name" inherits_from_parent="1" content_type="3" value="recording" default_value="" custom_choice="0" tooltip="Name of the file." id="4" />
                                                                                        <Parameter name="Temporary storage" inherits_from_parent="1" content_type="0" value="1" default_value="1" tooltip="Choose if the file should be stored as a temporary file so it is removed as soon&#x0A;as the behavior is unloaded.&#x0A;&#x0A;When this option is enabled the file is stored in ~/.cache/currentChoregrapheBehavior&#x0A;or in ~/.cache/&lt;project_name&gt; when you play the behavior from the&#x0A;behavior manager.&#x0A;&#x0A;When it is disabled the file is stored in ~/recordedSounds. You can&#x0A;then get it on your computer using the menu Connection/File transfer." id="5" />
                                                                                    </Box>
                                                                                    <Box name="Listening LEDS" id="2" localization="8" tooltip="" x="416" y="268">
                                                                                        <bitmap>media/images/box/box-python-script.png</bitmap>
                                                                                        <script language="4">
                                                                                            <content>
                                                                                                <![CDATA[import time

class MyClass(GeneratedClass):
    def __init__(self):
        GeneratedClass.__init__(self)

    def onLoad(self):
        self.leds = ALProxy("ALLeds")

    def onUnload(self):
        pass

    def onInput_onStart(self):
        #want RGB to be (31,88,255) -> 0x1F58FF
        self.leds.rotateEyes(2054399, 0.80, self.getParameter('Timeout (s)'))]]>
</content>
                                                                                        </script>
                                                                                        <Input name="onLoad" type="1" type_size="1" nature="0" inner="1" tooltip="Signal sent when diagram is loaded." id="1" />
                                                                                        <Input name="onStart" type="1" type_size="1" nature="2" inner="0" tooltip="Box behavior starts when a signal is received on this input." id="2" />
                                                                                        <Parameter name="Timeout (s)" inherits_from_parent="1" content_type="2" value="3" default_value="5" min="0.1" max="60" tooltip="Duration of the recording in seconds." id="3" />
                                                                                    </Box>
                                                                                    <Link inputowner="0" indexofinput="4" outputowner="4" indexofoutput="4" />
                                                                                    <Link inputowner="4" indexofinput="3" outputowner="13" indexofoutput="4" />
                                                                                    <Link inputowner="10" indexofinput="2" outputowner="0" indexofoutput="2" />
                                                                                    <Link inputowner="13" indexofinput="2" outputowner="10" indexofoutput="3" />
                                                                                    <Link inputowner="4" indexofinput="2" outputowner="10" indexofoutput="3" />
                                                                                    <Link inputowner="2" indexofinput="2" outputowner="10" indexofoutput="3" />
                                                                                </Diagram>
                                                                            </BehaviorKeyframe>
                                                                        </BehaviorLayer>
                                                                    </Timeline>
                                                                    <Resource name="Audio recorder" type="Lock" timeout="0" />
                                                                </Box>
                                                                <Link inputowner="5" indexofinput="2" outputowner="24" indexofoutput="4" />
                                                                <Link inputowner="24" indexofinput="2" outputowner="0" indexofoutput="2" />
                                                                <Link inputowner="0" indexofinput="3" outputowner="5" indexofoutput="3" />
                                                            </Diagram>
                                                        </BehaviorKeyframe>
                                                    </BehaviorLayer>
                                                </Timeline>
                                            </Box>
                                            <Link inputowner="6" indexofinput="2" outputowner="0" indexofoutput="2" />
                                            <Link inputowner="4" indexofinput="2" outputowner="6" indexofoutput="3" />
                                            <Link inputowner="5" indexofinput="2" outputowner="2" indexofoutput="3" />
                                            <Link inputowner="25" indexofinput="2" outputowner="4" indexofoutput="4" />
                                            <Link inputowner="25" indexofinput="2" outputowner="5" indexofoutput="4" />
                                            <Link inputowner="2" indexofinput="2" outputowner="25" indexofoutput="3" />
                                        </Diagram>
                                    </BehaviorKeyframe>
                                </BehaviorLayer>
                            </Timeline>
                        </Box>
                        <Link inputowner="7" indexofinput="2" outputowner="0" indexofoutput="2" />
                    </Diagram>
                </BehaviorKeyframe>
            </BehaviorLayer>
        </Timeline>
    </Box>
</ChoregrapheProject>
